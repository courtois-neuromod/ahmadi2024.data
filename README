<!-- vim: syntax=Markdown -->
# Courtois-NeuroMod: friends subdataset, derivatives timeseries for S.Ahmadi 2024 publication

The [Courtois project on Neural Modelling (Courtois NeuroMod)](https://www.cneuromod.ca/) aims at training artificial neural networks using extensive experimental data on individual human brain activity and behavior.

This data is related to the paper titled [Scaling ride regression for brain encoding in the large-scale dataset](https://arxiv.org/pdf/2403.19421.pdf). 

## Multiresolution Time Series Extraction

The fMRI time series includes three common spatial resolutions for each subject: parcel-wise, ROI-wise, and whole brain. These resolutions are described below:

1. **Parcels:**
   - The preprocessed BOLD time series were averaged across all voxels in each parcel of a parcellation atlas, using the NiftiLabelsMasker masker from Nilearn. We used the Multiresolution Intrinsic Segmentation Template (MIST) and the largest available resolution (444 brain parcels).

2. **ROI:**
   - A binary mask of the visual network was extracted from MIST at resolution 7. Voxel-wise time series were extracted for all voxels present in this mask, using the NiftiMasker masker from Nilearn. Note that the location of the mask was based on non-linear registration only, and did not use subject-specific segmentation of the grey matter. The exact same number of voxels (6728) was thus present in the mask for all subjectsâ€™ data.

3. **Whole Brain:**
   - The brain mask generated by the fMRIprep pipeline based on the structural scans of each participant was resampled at the resolution of the fMRI data. This mask included both grey matter, white matter, cerebro-spinal fluid but excluded all tissues surrounding the brain. Voxel-wise time series of all voxels included in the mask were extracted, again using the NiftiMasker masker from Nilearn. As the brain mask was subject-specific, the number of voxels in the mask varied slightly across subjects.

## Extracting VGG16 Features of Dynamic Visual Stimuli

For each fMRI time sample, we extracted the stimulus video frames corresponding to the 4 TRs immediately preceding each fMRI sample (equivalent to a window of 4 x 1.49 = 5.96s duration). This operation was done to take into account the known delayed, convolutional nature of the relationship between the visual stimulus and the hemodynamic response. Each frame was resampled to a (224, 224, 3) array and fed into VGG16 to extract 4096 features from the last layer. The features of the 4 TRs were concatenated, resulting in a single feature vector of length p = 16384.
